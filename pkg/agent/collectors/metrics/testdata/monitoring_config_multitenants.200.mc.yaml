# This file was generated by observability-operator.
# It configures Alloy to be used as a monitoring agent.
# - configMap is generated from logging.alloy.template and passed as a string
#   here and will be created by Alloy's chart.
# - Alloy runs as a statefulset, with required tolerations in order to scrape metrics
networkPolicy:
  cilium:
    egress:
    - toEntities:
      - kube-apiserver
      - cluster
      - world
    - toEndpoints:
      - matchLabels:
          k8s-app: coredns
      - matchLabels:
          k8s-app: k8s-dns-node-cache
      toPorts:
      - ports:
        - port: "1053"
          protocol: UDP
        - port: "1053"
          protocol: TCP
        - port: "53"
          protocol: UDP
        - port: "53"
          protocol: TCP
    - toEndpoints:
      - matchLabels:
          app.kubernetes.io/instance: alloy-metrics
          app.kubernetes.io/name: alloy
      toPorts:
      - ports:
        - port: "12345"
          protocol: TCP
    ingress:
    - fromEndpoints:
      - matchLabels:
          app.kubernetes.io/instance: alloy-metrics
          app.kubernetes.io/name: alloy
      toPorts:
      - ports:
        - port: "12345"
          protocol: TCP
alloy:
  alloy:
    clustering:
      enabled: true
    configMap:
      create: true
      content: |-
        
        logging {
          level  = "info"
          format = "logfmt"
        }
        
        remote.kubernetes.secret "credentials" {
          name      = "alloy-metrics"
          namespace = "kube-system"
        }
        // load rules for tenant tenant1
        mimir.rules.kubernetes "tenant1" {
          address = "http://mimir-ruler.mimir.svc:8080/"
          mimir_namespace_prefix = "dummy-cluster"
          tenant_id = "tenant1"
          rule_selector {
              match_labels = {
                "observability.giantswarm.io/tenant" = "tenant1",
              }
              match_expression {
                key = "application.giantswarm.io/prometheus-rule-kind"
                operator = "NotIn"
                values = ["loki"]
              }
          }
        }
        // load rules for tenant tenant2
        mimir.rules.kubernetes "tenant2" {
          address = "http://mimir-ruler.mimir.svc:8080/"
          mimir_namespace_prefix = "dummy-cluster"
          tenant_id = "tenant2"
          rule_selector {
              match_labels = {
                "observability.giantswarm.io/tenant" = "tenant2",
              }
              match_expression {
                key = "application.giantswarm.io/prometheus-rule-kind"
                operator = "NotIn"
                values = ["loki"]
              }
          }
        }
        
        // we create a podmonitor and servicemonitor component per tenant because we cannot read pod/service monitor labels through relabelling.
        // remote write pipeline configuration for tenant tenant1
        
        prometheus.operator.servicemonitors "tenant1" {
          forward_to = [prometheus.remote_write.tenant1.receiver]
          selector {
            match_expression {
              key      = "observability.giantswarm.io/tenant"
              operator = "In"
              values   = ["tenant1"]
            }
          }
          // Add the target's app.kubernetes.io/instance value into the metrics's app label
          // In the scope of servicemonitors the app.kubernetes.io/instance might be
          // exposed by 3 different Kubernetes resources.
          // The precedences order from highest to lowest is: Pod, Endpoint, Service.
          rule {
            action         =  "replace"
            regex          =  "(.+)"
            target_label   =  "app"
            source_labels  =  ["__meta_kubernetes_service_label_app_kubernetes_io_instance"]
          }
          rule {
            action         =  "replace"
            regex          =  "(.+)"
            target_label   =  "app"
            source_labels  =  ["__meta_kubernetes_endpoints_label_app_kubernetes_io_instance"]
          }
          rule {
            action         =  "replace"
            regex          =  "(.+)"
            target_label   =  "app"
            source_labels  =  ["__meta_kubernetes_pod_label_app_kubernetes_io_instance"]
          }
          scrape {
            default_scrape_interval = "60s"
          }
          clustering {
            enabled = true
          }
        }
        
        prometheus.operator.podmonitors "tenant1" {
          forward_to = [prometheus.remote_write.tenant1.receiver]
          selector {
            match_expression {
              key      = "observability.giantswarm.io/tenant"
              operator = "In"
              values   = ["tenant1"]
            }
          }
          // Add the Pod app.kubernetes.io/instance value into the metrics's app label
          rule {
            action         =  "replace"
            regex          =  "(.+)"
            target_label   =  "app"
            source_labels  =  ["__meta_kubernetes_pod_label_app_kubernetes_io_instance"]
          }
          scrape {
            default_scrape_interval = "60s"
          }
          clustering {
            enabled = true
          }
        }
        
        // remote write pipeline configuration for tenant tenant1
        prometheus.remote_write "tenant1" {
          endpoint {
            name           = nonsensitive(remote.kubernetes.secret.credentials.data["mimirRemoteWriteAPIName"])
            enable_http2   = false
            remote_timeout = "60s"
            url = "http://mimir-gateway.mimir.svc:80/api/v1/push"
        
            headers = {
              "X-Scope-OrgID" = "tenant1",
            }
            tls_config {
              insecure_skip_verify = false
            }
            queue_config {
              capacity             = 30000
              max_samples_per_send = 150000
              max_shards           = 10
              sample_age_limit     = "30m"
            }
          }
          wal {
            truncate_frequency = "1m0s"
          }
          external_labels = {
            "cluster_id" = "dummy-cluster",
            "cluster_type" = "management_cluster",
            "customer" = "dummy-customer",
            "installation" = "dummy-cluster",
            "organization" = "dummy-org",
            "pipeline" = "dummy-pipeline",
            "provider" = "capa",
            "region" = "dummy-region",
            "service_priority" = "highest",
          }
        }
        
        
        // remote write pipeline configuration for tenant tenant2
        
        prometheus.operator.servicemonitors "tenant2" {
          forward_to = [prometheus.remote_write.tenant2.receiver]
          selector {
            match_expression {
              key      = "observability.giantswarm.io/tenant"
              operator = "In"
              values   = ["tenant2"]
            }
          }
          // Add the target's app.kubernetes.io/instance value into the metrics's app label
          // In the scope of servicemonitors the app.kubernetes.io/instance might be
          // exposed by 3 different Kubernetes resources.
          // The precedences order from highest to lowest is: Pod, Endpoint, Service.
          rule {
            action         =  "replace"
            regex          =  "(.+)"
            target_label   =  "app"
            source_labels  =  ["__meta_kubernetes_service_label_app_kubernetes_io_instance"]
          }
          rule {
            action         =  "replace"
            regex          =  "(.+)"
            target_label   =  "app"
            source_labels  =  ["__meta_kubernetes_endpoints_label_app_kubernetes_io_instance"]
          }
          rule {
            action         =  "replace"
            regex          =  "(.+)"
            target_label   =  "app"
            source_labels  =  ["__meta_kubernetes_pod_label_app_kubernetes_io_instance"]
          }
          scrape {
            default_scrape_interval = "60s"
          }
          clustering {
            enabled = true
          }
        }
        
        prometheus.operator.podmonitors "tenant2" {
          forward_to = [prometheus.remote_write.tenant2.receiver]
          selector {
            match_expression {
              key      = "observability.giantswarm.io/tenant"
              operator = "In"
              values   = ["tenant2"]
            }
          }
          // Add the Pod app.kubernetes.io/instance value into the metrics's app label
          rule {
            action         =  "replace"
            regex          =  "(.+)"
            target_label   =  "app"
            source_labels  =  ["__meta_kubernetes_pod_label_app_kubernetes_io_instance"]
          }
          scrape {
            default_scrape_interval = "60s"
          }
          clustering {
            enabled = true
          }
        }
        
        // remote write pipeline configuration for tenant tenant2
        prometheus.remote_write "tenant2" {
          endpoint {
            name           = nonsensitive(remote.kubernetes.secret.credentials.data["mimirRemoteWriteAPIName"])
            enable_http2   = false
            remote_timeout = "60s"
            url = "http://mimir-gateway.mimir.svc:80/api/v1/push"
        
            headers = {
              "X-Scope-OrgID" = "tenant2",
            }
            tls_config {
              insecure_skip_verify = false
            }
            queue_config {
              capacity             = 30000
              max_samples_per_send = 150000
              max_shards           = 10
              sample_age_limit     = "30m"
            }
          }
          wal {
            truncate_frequency = "1m0s"
          }
          external_labels = {
            "cluster_id" = "dummy-cluster",
            "cluster_type" = "management_cluster",
            "customer" = "dummy-customer",
            "installation" = "dummy-cluster",
            "organization" = "dummy-org",
            "pipeline" = "dummy-pipeline",
            "provider" = "capa",
            "region" = "dummy-region",
            "service_priority" = "highest",
          }
        }
        
        
        
    # We decided to configure the alloy-metrics resources as such after some investigation done https://github.com/giantswarm/giantswarm/issues/32655#issuecomment-2729636063
    resources:
      limits:
        cpu: 100m
        memory: 768Mi
      requests:
        cpu: 25m
        memory: 512Mi
    ## Needed to to be able to support ScrapeConfig CRs
    stabilityLevel: experimental
  controller:
    type: statefulset
    replicas: 1
    priorityClassName: giantswarm-critical
  crds:
    create: false
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - preference:
          matchExpressions:
          - key: karpenter.sh/capacity-type
            operator: NotIn
            values:
            - spot
        weight: 100
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/name
              operator: In
              values:
              - alloy
          topologyKey: kubernetes.io/hostname
        weight: 50

# We decided to configure the alloy-metrics vertical pod autoscaler as such after some investigation done https://github.com/giantswarm/giantswarm/issues/32655#issuecomment-2729636063
verticalPodAutoscaler:
  enabled: true
  resourcePolicy:
    containerPolicies:
    - containerName: alloy
      controlledResources:
      - memory
      - cpu
      controlledValues: "RequestsAndLimits"
      maxAllowed:
        cpu: 500m
        memory: 12Gi
      minAllowed:
        cpu: 25m
        memory: 512Mi
