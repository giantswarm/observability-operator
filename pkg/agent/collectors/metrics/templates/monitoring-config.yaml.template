# This file was generated by observability-operator.
# It configures Alloy to be used as a monitoring agent.
# - configMap is generated from logging.alloy.template and passed as a string
#   here and will be created by Alloy's chart.
# - Alloy runs as a statefulset, with required tolerations in order to scrape metrics
networkPolicy:
  cilium:
    egress:
    - toEntities:
      - kube-apiserver
      - cluster
      - world
    - toEndpoints:
      - matchLabels:
          k8s-app: coredns
      - matchLabels:
          k8s-app: k8s-dns-node-cache
      toPorts:
      - ports:
        - port: "1053"
          protocol: UDP
        - port: "1053"
          protocol: TCP
        - port: "53"
          protocol: UDP
        - port: "53"
          protocol: TCP
    - toEndpoints:
      - matchLabels:
          app.kubernetes.io/instance: alloy-metrics
          app.kubernetes.io/name: alloy
      toPorts:
      - ports:
        - port: "12345"
          protocol: TCP
    ingress:
    - fromEndpoints:
      - matchLabels:
          app.kubernetes.io/instance: alloy-metrics
          app.kubernetes.io/name: alloy
      toPorts:
      - ports:
        - port: "12345"
          protocol: TCP
alloy:
  alloy:
    clustering:
      enabled: true
    configMap:
      create: true
      content: |-
        {{ .AlloyConfig | nindent 8 }}
    # We decided to configure the alloy-metrics resources as such after some investigation done https://github.com/giantswarm/giantswarm/issues/32655#issuecomment-2729636063
    resources:
      limits:
        cpu: 100m
        memory: 768Mi
      requests:
        cpu: 25m
        memory: 512Mi
    ## Needed to to be able to support ScrapeConfig CRs
    stabilityLevel: experimental
  controller:
    type: statefulset
    replicas: {{ .Replicas }}
    priorityClassName: {{ .PriorityClassName }}
  crds:
    create: false
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - preference:
          matchExpressions:
          - key: karpenter.sh/capacity-type
            operator: NotIn
            values:
            - spot
        weight: 100
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/name
              operator: In
              values:
              - alloy
          topologyKey: kubernetes.io/hostname
        weight: 50

{{- if .KEDAAuthenticationEnabled }}
  extraObjects:
  - apiVersion: keda.sh/v1alpha1
    kind: ClusterTriggerAuthentication
    metadata:
      name: mimir-auth
      labels:
        app.kubernetes.io/managed-by: observability-operator
        giantswarm.io/service-type: managed
    spec:
      secretTargetRef:
        - parameter: username
          name: {{ .AlloySecretName }}
          key: {{ .MimirRemoteWriteAPIUsernameKey }}
        - parameter: password
          name: {{ .AlloySecretName }}
          key: {{ .MimirRemoteWriteAPIPasswordKey }}
{{- end }}

# We decided to configure the alloy-metrics vertical pod autoscaler as such after some investigation done https://github.com/giantswarm/giantswarm/issues/32655#issuecomment-2729636063
verticalPodAutoscaler:
  enabled: true
  resourcePolicy:
    containerPolicies:
    - containerName: alloy
      controlledResources:
      - memory
      - cpu
      controlledValues: "RequestsAndLimits"
      maxAllowed:
        cpu: 500m
        memory: 12Gi
      minAllowed:
        cpu: 25m
        memory: 512Mi
