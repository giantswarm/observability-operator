# This file was generated by observability-operator.
# It configures Alloy to be used as events logger.
# - configMap is generated from events-logger.alloy.template and passed as a string
#   here and will be created by Alloy's chart.
# - Alloy runs as a deployment, with only 1 replica.
{{- if or .TracingEnabled (not .IsWorkloadCluster) }}
networkPolicy:
  cilium:
    {{- if not .IsWorkloadCluster }}
    egress:
    - toEntities:
      - kube-apiserver
      - world
    # Allow direct access to loki-gateway
    - toEndpoints:
      - matchLabels:
          app.kubernetes.io/component: gateway
          app.kubernetes.io/name: loki
          io.kubernetes.pod.namespace: loki
      toPorts:
      - ports:
        - port: "8080"
          protocol: TCP
    - toEndpoints:
      - matchLabels:
          k8s-app: coredns
      - matchLabels:
          k8s-app: k8s-dns-node-cache
      toPorts:
      - ports:
        - port: "53"
          protocol: ANY
        - port: "1053"
          protocol: ANY
        rules:
          dns:
            - matchPattern: '*'
    - toEndpoints:
      - matchLabels:
          app.kubernetes.io/name: ingress-nginx
      toPorts:
      - ports:
        - port: "80"
          protocol: ANY
        - port: "443"
          protocol: ANY
    - toEndpoints:
      - matchLabels:
          app.kubernetes.io/instance: alloy-events
          app.kubernetes.io/name: alloy
      toPorts:
      - ports:
        - port: "12345"
          protocol: TCP
    {{- if .TracingEnabled }}
    # Allow direct access to tempo
    - toEndpoints:
      - matchLabels:
          app.kubernetes.io/name: tempo
          io.kubernetes.pod.namespace: tempo
      toPorts:
      - ports:
        - port: "4317"
          protocol: TCP
        - port: "4318"
          protocol: TCP
    {{- end }}
    {{- end }}
    {{- if .TracingEnabled }}
    ingress:
    - toPorts:
      - ports:
        # Alloy Control Plane
        - port: "12345"
          protocol: TCP
        # OTLP GRPC
        - port: "4317"
          protocol: "TCP"
        # OTLP HTTP
        - port: "4318"
          protocol: "TCP"
    {{- end }}
{{- end }}
alloy:
  alloy:
    configMap:
      create: true
      content: |-
        {{ .AlloyConfig | nindent 8 | replace "        \n" "" | trim }}
    # We decided to configure the alloy-events resources as such after some investigation done https://github.com/giantswarm/giantswarm/issues/32655
    # We also updated the alloy-events CPU request and limits here https://github.com/giantswarm/giantswarm/issues/34619 to avoid CPU throttling
    resources:
      limits:
        cpu: 500m
        memory: 256Mi
      requests:
        cpu: 50m
        memory: 128Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: false
      runAsUser: 10
      runAsGroup: 10
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
  controller:
    type: deployment
    replicas: 1
  crds:
    create: false
{{- if .TracingEnabled }}
  extraObjects:
  # OTLP gateway service to receive traces
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        meta.helm.sh/release-name: alloy-events
        meta.helm.sh/release-namespace: kube-system
      labels:
        app.kubernetes.io/component: networking
        app.kubernetes.io/instance: alloy-events
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: alloy
        app.kubernetes.io/part-of: alloy
        application.giantswarm.io/team: atlas
        giantswarm.io/managed-by: alloy-events
        giantswarm.io/service-type: managed
        helm.sh/chart: alloy-1.1.0
      name: otlp-gateway
      namespace: kube-system
    spec:
      ports:
      - appProtocol: grpc
        name: otlp
        port: 4317
        protocol: TCP
        targetPort: 4317
      - appProtocol: http
        name: otlp-http
        port: 4318
        protocol: TCP
        targetPort: 4318
      selector:
        app.kubernetes.io/instance: alloy-events
        app.kubernetes.io/name: alloy
      type: ClusterIP
{{- end }}
verticalPodAutoscaler:
  enabled: true
  # We decided to configure the alloy-events vertical pod autoscaler as such after some investigation done https://github.com/giantswarm/giantswarm/issues/32655
  resourcePolicy:
    containerPolicies:
    - containerName: alloy
      controlledResources:
      - memory
      controlledValues: "RequestsAndLimits"
