# This file was generated by observability-operator.
# It configures Alloy to be used as a logging agent.
# - configMap is generated from logging.alloy.template and passed as a string
#   here and will be created by Alloy's chart.
# - Alloy runs as a daemonset, with required tolerations in order to scrape logs
#   from every machine in the cluster.
# - Running as root user is required in order to be able to read log files within
#   /run/log/journal directories.
# - NODE_NAME env var is used as additional label for kubernetes_audit logs.
networkPolicy:
  cilium:
    egress:
    - toEntities:
      - kube-apiserver
      - world
    - toEndpoints:
      - matchLabels:
          io.kubernetes.pod.namespace: kube-system
          k8s-app: coredns
      - matchLabels:
          io.kubernetes.pod.namespace: kube-system
          k8s-app: k8s-dns-node-cache
      toPorts:
      - ports:
        - port: "1053"
          protocol: UDP
        - port: "1053"
          protocol: TCP
        - port: "53"
          protocol: UDP
        - port: "53"
          protocol: TCP
  endpointSelector:
    matchLabels:
      app.kubernetes.io/instance: alloy-logs
      app.kubernetes.io/name: alloy

alloy:
  alloy:
    configMap:
      create: true
      content: |-
        logging {
        	level  = "warn"
        	format = "logfmt"
        }
        remote.kubernetes.secret "credentials" {
        	namespace = "kube-system"
        	name = "alloy-logs"
        }
        beyla.ebpf "default" {
        	attributes {
        		kubernetes {
        			cluster_name = "test-cluster"
        			enable = "true"
        		}
        		select {
        			attr = "beyla_network_flow_bytes"
        			include = [
        				"k8s.src.namespace",
        				"k8s.src.name",
        				"k8s.src.type",
        				"k8s.dst.namespace",
        				"k8s.dst.name",
        				"k8s.dst.type",
        				"src.cidr",
        				"src.address",
        				"src.name",
        				"dst.cidr",
        				"dst.address",
        				"src.zone",
        				"dst.zone",
        				"dst.name",
        				"transport",
        				"direction",
        			]
        		}
        	}
        	filters {
        		network {
        			attr = "direction"
        			match = "request"
        		}
        	}
        	metrics {
        		features = [
        			"network",
        			"network_inter_zone",
        		]
        	}
        }
        // load rules for tenant giantswarm
        loki.rules.kubernetes "giantswarm" {
        	address = convert.nonsensitive(remote.kubernetes.secret.credentials.data["ruler-api-url"])
        	basic_auth {
        		username = convert.nonsensitive(remote.kubernetes.secret.credentials.data["logging-username"])
        		password = remote.kubernetes.secret.credentials.data["logging-password"]
        	}
        	loki_namespace_prefix = "test-cluster"
        	tenant_id = "giantswarm"
        	rule_selector {
        		match_labels = {
        			"observability.giantswarm.io/tenant" = "giantswarm",
        		}
        		match_expression {
        			key = "application.giantswarm.io/prometheus-rule-kind"
        			operator = "In"
        			values = ["loki"]
        		}
        	}
        }
        // Native podlogs collection (preferred method for scalability)
        loki.source.podlogs "kubernetes_pods" {
        	forward_to = [loki.relabel.kubernetes_pods.receiver]
        	node_filter {
        		enabled = true
        		node_name = sys.env("NODE_NAME")
        	}
        }
        loki.relabel "kubernetes_pods" {
        	forward_to = [loki.process.kubernetes_pods.receiver]
        	rule {
        		target_label = "scrape_job"
        		replacement  = "kubernetes-pods"
        	}
        	// Extract namespace, pod, and container from the structured instance label
        	// Format: "namespace/pod:container" (e.g., "kube-system/mimir-distributor-abc123:mimir")
        	rule {
        		source_labels = ["instance"]
        		regex         = "([^/]+)/.+"
        		target_label  = "namespace"
        	}
        	rule {
        		source_labels = ["instance"]
        		regex         = "[^/]+/([^:]+):.+"
        		target_label  = "pod"
        	}
        	rule {
        		source_labels = ["instance"]
        		regex         = "[^/]+/[^:]+:(.+)"
        		target_label  = "container"
        	}
        	// Extract tenant ID for authorized tenants only - logs from unauthorized
        	// tenants will be dropped later in the processing pipeline
        	// Configured tenants: giantswarm
        	rule {
        		source_labels = ["giantswarm_observability_tenant"]
        		regex         = "^(giantswarm)$"
        		target_label  = "__tenant_id__"
        	}
        	// Remove the source tenant label to keep Loki labels clean
        	rule {
        		regex  = "giantswarm_observability_tenant"
        		action = "labeldrop"
        	}
        	// Extract and normalize standard k8s labels with priority-based fallbacks
        	// Priority: app.kubernetes.io/name > app > pod name (pod logs then file-based discovery)
        	rule {
        		source_labels = ["app_kubernetes_io_name", "app", "pod", "__meta_kubernetes_pod_name"]
        		regex         = "^;*([^;]+)(;.*)?$"
        		target_label  = "app"
        	}
        	rule {
        		source_labels = ["app_kubernetes_io_component", "component"]
        		regex         = "^;*([^;]+)(;.*)?$"
        		target_label  = "component"
        	}
        	rule {
        		source_labels = ["app_kubernetes_io_version", "version"]
        		regex         = "^;*([^;]+)(;.*)?$"
        		target_label  = "version"
        	}
        	// Create unified service name by combining app + component to align Loki and Tempo signals
        	// Only creates service label when BOTH app and component are non-empty
        	// Handles app names with hyphens like "alertmanager-to-github" or "background-controller"
        	// Examples: "mimir" + "distributor" → "mimir-distributor" (matches Tempo service.name)
        	//           "alertmanager-to-github" + "webhook" → "alertmanager-to-github-webhook"
        	rule {
        		source_labels = ["app", "component"]
        		regex         = "^(.+);(.+)$"
        		replacement   = "${1}-${2}"
        		target_label  = "service"
        	}
        	rule {
        		regex  = "app_kubernetes_io_(component|name|version)"
        		action = "labeldrop"
        	}
        }
        loki.process "kubernetes_pods" {
        	forward_to = [loki.write.default.receiver]
        	// Parse container runtime interface (CRI) log format
        	stage.cri { }
        	// Multi-tenant filtering: drop logs without valid tenant authorization
        	stage.drop {
        		drop_counter_reason = "no_tenant_id"
        		source              = "__tenant_id__"
        		expression          = "^$"
        	}
        	// Move high-cardinality metadata to structured metadata instead of labels
        	stage.structured_metadata {
        		values = {
        			"filename" = "",
        			"stream" = "",
        		}
        	}
        	// Clean up temporary labels used only for processing
        	stage.label_drop {
        		values = [
        			"filename",
        			"stream",
        		]
        	}
        }
        // journald logs from /run/log/journal
        loki.process "systemd_journal_run" {
        	forward_to = [loki.write.default.receiver]
        	stage.json {
        		expressions = {
        			SYSLOG_IDENTIFIER = "SYSLOG_IDENTIFIER",
        		}
        	}
        	stage.drop {
        		source = "SYSLOG_IDENTIFIER"
        		value  = "audit"
        	}
        }
        discovery.relabel "systemd_journal_run" {
        	targets = []
        	rule {
        		source_labels = ["__journal__systemd_unit"]
        		target_label  = "__tmp_systemd_unit"
        	}
        	rule {
        		source_labels = ["__journal__systemd_unit", "__journal_syslog_identifier"]
        		regex         = ";(.+)"
        		target_label  = "__tmp_systemd_unit"
        	}
        	rule {
        		source_labels = ["__tmp_systemd_unit"]
        		target_label  = "systemd_unit"
        	}
        	rule {
        		source_labels = ["__journal__hostname"]
        		target_label  = "node"
        	}
        }
        loki.source.journal "systemd_journal_run" {
        	format_as_json = true
        	max_age        = "12h0m0s"
        	path           = "/run/log/journal"
        	relabel_rules  = discovery.relabel.systemd_journal_run.rules
        	forward_to     = [loki.process.systemd_journal_run.receiver]
        	labels         = {
        		scrape_job = "system-logs",
        	}
        }
        // Kubernetes API server audit logs
        local.file_match "kubernetes_audit" {
        	path_targets = [{
        		__address__ = "localhost",
        		__path__    = "/var/log/apiserver/audit.log",
        		node   = coalesce(sys.env("NODE_NAME"), "unknown"),
        		scrape_job  = "audit-logs",
        	}]
        }
        loki.process "kubernetes_audit" {
        	forward_to = [loki.write.default.receiver]
        	stage.json {
        		expressions = {
        			objectRef = "objectRef",
        		}
        	}
        	stage.json {
        		expressions = {
        			namespace = "namespace",
        			resource  = "resource",
        		}
        		source = "objectRef"
        	}
        	stage.structured_metadata {
        		values = {
        			"resource" = "",
        			"filename" = "",
        		}
        	}
        	stage.label_drop {
        		values = [
        			"filename",
        		]
        	}
        	stage.labels {
        		values = {
        			namespace = "",
        		}
        	}
        }
        loki.source.file "kubernetes_audit" {
        	targets               = local.file_match.kubernetes_audit.targets
        	forward_to            = [loki.process.kubernetes_audit.receiver]
        	legacy_positions_file = "/run/alloy/positions.yaml"
        }
        // Loki target configuration
        loki.write "default" {
        	endpoint {
        		basic_auth {
        			username = convert.nonsensitive(remote.kubernetes.secret.credentials.data["logging-username"])
        			password = remote.kubernetes.secret.credentials.data["logging-password"]
        		}
        		url                = convert.nonsensitive(remote.kubernetes.secret.credentials.data["logging-url"])
        		max_backoff_period = "10m"
        		remote_timeout     = "60s"
        		tenant_id          = convert.nonsensitive(remote.kubernetes.secret.credentials.data["logging-tenant-id"])
        		tls_config {
        			insecure_skip_verify = false
        		}
        	}
        	external_labels = {
        		cluster_id       = "test-cluster",
        		cluster_type     = "workload_cluster",
        		organization     = "test-organization",
        		provider         = "capa",
        	}
        }
    clustering:
      enabled: false
      name: alloy-logs
    extraEnv:
    - name: NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    mounts:
      varlog: true
      dockercontainers: true
      extra:
      - name: runlogjournal
        mountPath: /run/log/journal
        readOnly: true
      # This is needed to allow alloy to create files when using readOnlyRootFilesystem
      - name: alloy-tmp
        mountPath: /tmp/alloy
    # We decided to configure the alloy-logs resources as such after some investigation done https://github.com/giantswarm/giantswarm/issues/32655
    resources:
      limits:
        cpu: 2000m
        memory: 300Mi
      requests:
        cpu: 25m
        memory: 200Mi
    securityContext:
      allowPrivilegeEscalation: true
      appArmorProfile:
        type: Unconfined
      capabilities:
        add:
        - BPF
        - CHECKPOINT_RESTORE
        - DAC_READ_SEARCH
        - NET_RAW
        - NET_ADMIN
        - PERFMON
        - SYS_PTRACE
        - SYS_RESOURCE
        - SYS_ADMIN
        drop: []
      privileged: true
      readOnlyRootFilesystem: true
      runAsUser: 0
      runAsGroup: 0
      runAsNonRoot: false
      seccompProfile:
        type: Unconfined
  controller:
    type: daemonset
    hostPID: true
    hostNetwork: true
    priorityClassName: giantswarm-critical
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    volumes:
      extra:
      - name: runlogjournal
        hostPath:
          path: /run/log/journal
      - name: alloy-tmp
        emptyDir: {}
  image:
    tag: v1.12.0
  extraObjects:
  - apiVersion: monitoring.coreos.com/v1
    kind: ServiceMonitor
    metadata:
      labels:
        observability.giantswarm.io/tenant: giantswarm
      name: alloy-logs-beyla
      namespace: kube-system
    spec:
      endpoints:
      - honorLabels: true
        port: http-metrics
        path: /api/v0/component/beyla.ebpf.default/metric
        scheme: http
      selector:
        matchLabels:
          app.kubernetes.io/instance: alloy-logs
          app.kubernetes.io/name: alloy

verticalPodAutoscaler:
  enabled: true
  # We decided to configure the alloy-logs vertical pod autoscaler as such after some investigation done https://github.com/giantswarm/giantswarm/issues/32655
  resourcePolicy:
    containerPolicies:
    - containerName: alloy
      controlledResources:
      - memory
      controlledValues: "RequestsAndLimits"
      maxAllowed:
        memory: 1Gi
podLogs:
- name: default-namespaces
  namespace: kube-system
  spec:
    selector: {}
    namespaceSelector:
      matchExpressions:
      - key: kubernetes.io/metadata.name
        operator: In
        values:
        - test-selector
    relabelings:
    - action: replace
      targetLabel: "giantswarm_observability_tenant"
      replacement: giantswarm
    - action: replace
      sourceLabels: ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
      targetLabel: "app_kubernetes_io_name"
    - action: replace
      sourceLabels: ["__meta_kubernetes_pod_label_app_kubernetes_io_component"]
      targetLabel: "app_kubernetes_io_component"
    - action: replace
      sourceLabels: ["__meta_kubernetes_pod_label_app_kubernetes_io_version"]
      targetLabel: "app_kubernetes_io_version"
- name: customers-logs
  namespace: kube-system
  spec:
    selector:
      matchExpressions:
      - key: observability.giantswarm.io/tenant
        operator: Exists
    namespaceSelector:
      matchExpressions:
      - key: kubernetes.io/metadata.name
        operator: NotIn
        values:
        - test-selector
    relabelings:
    - action: replace
      sourceLabels: ["__meta_kubernetes_pod_label_observability_giantswarm_io_tenant"]
      targetLabel: "giantswarm_observability_tenant"
    - action: replace
      sourceLabels: ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
      targetLabel: "app_kubernetes_io_name"
    - action: replace
      sourceLabels: ["__meta_kubernetes_pod_label_app_kubernetes_io_component"]
      targetLabel: "app_kubernetes_io_component"
    - action: replace
      sourceLabels: ["__meta_kubernetes_pod_label_app_kubernetes_io_version"]
      targetLabel: "app_kubernetes_io_version"
kyvernoPolicyExceptions:
  enabled: true
  namespace: giantswarm
  exceptions:
  - policyName: restrict-volume-types
    ruleNames:
    - "*"
  - policyName: always-allow-heartbeats-and-all-pipelines-alerts
    ruleNames:
    - "*"
  - policyName: block-k8s-initiator-app-deployment-capa
    ruleNames:
    - "*"
  - policyName: disallow-capabilities
    ruleNames:
    - "*"
  - policyName: disallow-capabilities-strict
    ruleNames:
    - "*"
  - policyName: disallow-host-namespaces
    ruleNames:
    - "*"
  - policyName: disallow-host-path
    ruleNames:
    - "*"
  - policyName: disallow-host-ports
    ruleNames:
    - "*"
  - policyName: disallow-host-process
    ruleNames:
    - "*"
  - policyName: disallow-noisy-policy-contexts
    ruleNames:
    - "*"
  - policyName: disallow-privilege-escalation
    ruleNames:
    - "*"
  - policyName: disallow-privileged-containers
    ruleNames:
    - "*"
  - policyName: disallow-proc-mount
    ruleNames:
    - "*"
  - policyName: disallow-selinux
    ruleNames:
    - "*"
  - policyName: require-emptydir-requests-and-limits
    ruleNames:
    - "*"
  - policyName: require-run-as-non-root-user
    ruleNames:
    - "*"
  - policyName: require-run-as-nonroot
    ruleNames:
    - "*"
  - policyName: restrict-apparmor-profiles
    ruleNames:
    - "*"
  - policyName: restrict-polex-namespaces
    ruleNames:
    - "*"
  - policyName: restrict-policy-kind-wildcards
    ruleNames:
    - "*"
  - policyName: restrict-seccomp
    ruleNames:
    - "*"
  - policyName: restrict-seccomp-strict
    ruleNames:
    - "*"
  - policyName: restrict-sysctls
    ruleNames:
    - "*"
  - policyName: restrict-volume-types
    ruleNames:
    - "*"
