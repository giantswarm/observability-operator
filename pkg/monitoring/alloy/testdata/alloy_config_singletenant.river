logging {
  level  = "info"
  format = "logfmt"
}

// we create a podmonitor and servicemonitor component per tenant because we cannot read pod/service monitor labels through relabelling.

// remote write pipeline configuration for tenant tenant1

prometheus.operator.servicemonitors "tenant1" {
  forward_to = [prometheus.remote_write.tenant1.receiver]
  selector {
    match_expression {
      key      = "observability.giantswarm.io/tenant"
      operator = "In"
      values   = ["tenant1"]
    }
  }
  scrape {
    default_scrape_interval = "60s"
  }
  clustering {
    enabled = true
  }
}

prometheus.operator.podmonitors "tenant1" {
  forward_to = [prometheus.remote_write.tenant1.receiver]
  selector {
    match_expression {
      key      = "observability.giantswarm.io/tenant"
      operator = "In"
      values   = ["tenant1"]
    }
  }
  scrape {
    default_scrape_interval = "60s"
  }
  clustering {
    enabled = true
  }
}

// remote write pipeline configuration for tenant tenant1
prometheus.remote_write "tenant1" {
  endpoint {
    url            = env("REMOTE_WRITE_URL")
    name           = env("REMOTE_WRITE_NAME")
    enable_http2   = false
    remote_timeout = "60s"
    basic_auth {
      username = env("BASIC_AUTH_USERNAME")
      password = env("BASIC_AUTH_PASSWORD")
    }
    headers = {
      "X-Scope-OrgID" = "tenant1",
    }
    tls_config {
      insecure_skip_verify = false
    }
    queue_config {
      capacity             = 30000
      max_shards           = 10
      max_samples_per_send = 150000
      sample_age_limit     = "30m"
    }
  }
  wal {
    truncate_frequency = "1m0s"
  }
  external_labels = {
    "cluster_id" = "single-tenant-cluster",
    "cluster_type" = "workload_cluster",
    "customer" = "dummy-customer",
    "installation" = "dummy-cluster",
    "organization" = "dummy-org",
    "pipeline" = "dummy-pipeline",
    "provider" = "capz",
    "region" = "dummy-region",
    "service_priority" = "highest",
  }
}


