logging {
  level  = "info"
  format = "logfmt"
}

remote.kubernetes.secret "credentials" {
  name      = "alloy-metrics"
  namespace = "kube-system"
}
// load rules for tenant tenant1
mimir.rules.kubernetes "tenant1" {
  address = "http://mimir-ruler.mimir.svc:80/"
  mimir_namespace_prefix = "dummy-cluster"
  tenant_id = "tenant1"
  rule_selector {
      match_labels = {
        "observability.giantswarm.io/tenant" = "tenant1",
      }
      match_expression {
        key = "application.giantswarm.io/prometheus-rule-kind"
        operator = "NotIn"
        values = ["loki"]
      }
  }
}
// load rules for tenant tenant2
mimir.rules.kubernetes "tenant2" {
  address = "http://mimir-ruler.mimir.svc:80/"
  mimir_namespace_prefix = "dummy-cluster"
  tenant_id = "tenant2"
  rule_selector {
      match_labels = {
        "observability.giantswarm.io/tenant" = "tenant2",
      }
      match_expression {
        key = "application.giantswarm.io/prometheus-rule-kind"
        operator = "NotIn"
        values = ["loki"]
      }
  }
}

// we create a podmonitor and servicemonitor component per tenant because we cannot read pod/service monitor labels through relabelling.
// remote write pipeline configuration for tenant tenant1

prometheus.operator.servicemonitors "tenant1" {
  forward_to = [prometheus.remote_write.tenant1.receiver]
  selector {
    match_expression {
      key      = "observability.giantswarm.io/tenant"
      operator = "In"
      values   = ["tenant1"]
    }
  }
  scrape {
    default_scrape_interval = "60s"
  }
  clustering {
    enabled = true
  }
}

prometheus.operator.podmonitors "tenant1" {
  forward_to = [prometheus.remote_write.tenant1.receiver]
  selector {
    match_expression {
      key      = "observability.giantswarm.io/tenant"
      operator = "In"
      values   = ["tenant1"]
    }
  }
  scrape {
    default_scrape_interval = "60s"
  }
  clustering {
    enabled = true
  }
}

// remote write pipeline configuration for tenant tenant1
prometheus.remote_write "tenant1" {
  endpoint {
    name           = nonsensitive(remote.kubernetes.secret.credentials.data["mimirRemoteWriteAPIName"])
    enable_http2   = false
    remote_timeout = "60s"
    url = "http://mimir-gateway.mimir.svc:8080/api/v1/push"

    headers = {
      "X-Scope-OrgID" = "tenant1",
    }
    tls_config {
      insecure_skip_verify = false
    }
    queue_config {
      capacity             = 30000
      max_samples_per_send = 150000
      max_shards           = 10
      sample_age_limit     = "30m"
    }
  }
  wal {
    truncate_frequency = "1m0s"
  }
  external_labels = {
    "cluster_id" = "dummy-cluster",
    "cluster_type" = "management_cluster",
    "customer" = "dummy-customer",
    "installation" = "dummy-cluster",
    "organization" = "dummy-org",
    "pipeline" = "dummy-pipeline",
    "provider" = "capa",
    "region" = "dummy-region",
    "service_priority" = "highest",
  }
}


// remote write pipeline configuration for tenant tenant2

prometheus.operator.servicemonitors "tenant2" {
  forward_to = [prometheus.remote_write.tenant2.receiver]
  selector {
    match_expression {
      key      = "observability.giantswarm.io/tenant"
      operator = "In"
      values   = ["tenant2"]
    }
  }
  scrape {
    default_scrape_interval = "60s"
  }
  clustering {
    enabled = true
  }
}

prometheus.operator.podmonitors "tenant2" {
  forward_to = [prometheus.remote_write.tenant2.receiver]
  selector {
    match_expression {
      key      = "observability.giantswarm.io/tenant"
      operator = "In"
      values   = ["tenant2"]
    }
  }
  scrape {
    default_scrape_interval = "60s"
  }
  clustering {
    enabled = true
  }
}

// remote write pipeline configuration for tenant tenant2
prometheus.remote_write "tenant2" {
  endpoint {
    name           = nonsensitive(remote.kubernetes.secret.credentials.data["mimirRemoteWriteAPIName"])
    enable_http2   = false
    remote_timeout = "60s"
    url = "http://mimir-gateway.mimir.svc:8080/api/v1/push"

    headers = {
      "X-Scope-OrgID" = "tenant2",
    }
    tls_config {
      insecure_skip_verify = false
    }
    queue_config {
      capacity             = 30000
      max_samples_per_send = 150000
      max_shards           = 10
      sample_age_limit     = "30m"
    }
  }
  wal {
    truncate_frequency = "1m0s"
  }
  external_labels = {
    "cluster_id" = "dummy-cluster",
    "cluster_type" = "management_cluster",
    "customer" = "dummy-customer",
    "installation" = "dummy-cluster",
    "organization" = "dummy-org",
    "pipeline" = "dummy-pipeline",
    "provider" = "capa",
    "region" = "dummy-region",
    "service_priority" = "highest",
  }
}


