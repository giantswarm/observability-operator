logging {
  level  = "info"
  format = "logfmt"
}

{{- if $.IsSupportingExtraQueryMatchers }}
{{- range .Tenants }}
// load rules for tenant {{ . }}
mimir.rules.kubernetes "{{ . }}" {
  {{- if $.IsWorkloadCluster }}
  address = env("{{ $.RulerAPIURLEnvVarName }}")
  basic_auth {
    username = env("{{ $.RemoteWriteBasicAuthUsernameEnvVarName }}")
    password = env("{{ $.RemoteWriteBasicAuthPasswordEnvVarName }}")
  }
  extra_query_matchers {
    matcher {
        name = "cluster_id"
        match_type = "="
        value = "{{ $.ClusterID }}"
    }
  }
  {{- else }}
  address = "http://mimir-ruler.mimir.svc:8080/"
  {{- end }}
  mimir_namespace_prefix = "{{ $.ClusterID }}"
  tenant_id = "{{ . }}"
  rule_selector {
      match_labels = {
        "observability.giantswarm.io/tenant" = "{{ . }}",
      }
      match_expression {
        key = "application.giantswarm.io/prometheus-rule-kind"
        operator = "NotIn"
        values = ["loki"]
      }
  }
}
{{- end }}
{{- end }}

// we create a podmonitor and servicemonitor component per tenant because we cannot read pod/service monitor labels through relabelling.
{{- range .Tenants }}
// remote write pipeline configuration for tenant {{ . }}
{{- if eq . $.DefaultTenantID }}
prometheus.operator.servicemonitors "{{ . }}_legacy" {
  forward_to = [prometheus.remote_write.{{ . }}.receiver]
  selector {
    match_expression {
      key      = "application.giantswarm.io/team"
      operator = "Exists"
    }
    match_expression {
      key      = "observability.giantswarm.io/tenant"
      operator = "DoesNotExist"
    }
  }
  scrape {
    default_scrape_interval = "60s"
  }
  clustering {
    enabled = true
  }
}

prometheus.operator.podmonitors "{{ . }}_legacy" {
  forward_to = [prometheus.remote_write.{{ . }}.receiver]
  selector {
    match_expression {
      key      = "application.giantswarm.io/team"
      operator = "Exists"
    }
    match_expression {
      key      = "observability.giantswarm.io/tenant"
      operator = "DoesNotExist"
    }
  }
  scrape {
    default_scrape_interval = "60s"
  }
  clustering {
    enabled = true
  }
}
{{ end }}

prometheus.operator.servicemonitors "{{ . }}" {
  forward_to = [prometheus.remote_write.{{ . }}.receiver]
  selector {
    match_expression {
      key      = "observability.giantswarm.io/tenant"
      operator = "In"
      values   = ["{{ . }}"]
    }
  }
  scrape {
    default_scrape_interval = "60s"
  }
  clustering {
    enabled = true
  }
}

prometheus.operator.podmonitors "{{ . }}" {
  forward_to = [prometheus.remote_write.{{ . }}.receiver]
  selector {
    match_expression {
      key      = "observability.giantswarm.io/tenant"
      operator = "In"
      values   = ["{{ . }}"]
    }
  }
  scrape {
    default_scrape_interval = "60s"
  }
  clustering {
    enabled = true
  }
}

// remote write pipeline configuration for tenant {{ . }}
prometheus.remote_write "{{ . }}" {
  endpoint {
    url            = env("{{ $.RemoteWriteURLEnvVarName }}")
    name           = env("{{ $.RemoteWriteNameEnvVarName }}")
    enable_http2   = false
    remote_timeout = "{{ $.RemoteWriteTimeout }}"
    basic_auth {
      username = env("{{ $.RemoteWriteBasicAuthUsernameEnvVarName }}")
      password = env("{{ $.RemoteWriteBasicAuthPasswordEnvVarName }}")
    }
    headers = {
      "X-Scope-OrgID" = "{{ . }}",
    }
    tls_config {
      insecure_skip_verify = {{ $.RemoteWriteTLSInsecureSkipVerify }}
    }
    queue_config {
      capacity             = {{ $.QueueConfigCapacity }}
      max_shards           = {{ $.QueueConfigMaxShards }}
      max_samples_per_send = {{ $.QueueConfigMaxSamplesPerSend }}
      sample_age_limit     = "{{ $.QueueConfigSampleAgeLimit }}"
    }
  }
  wal {
    truncate_frequency = "{{ $.WALTruncateFrequency }}"
  }
  external_labels = {
    {{- range $key, $value := $.ExternalLabels }}
    "{{ $key }}" = "{{ $value }}",
    {{- end }}
  }
}

{{ end }}
