# This file was generated by observability-operator.
# It configures Alloy to be used as a monitoring agent.
# - configMap is generated from logging.alloy.template and passed as a string
#   here and will be created by Alloy's chart.
# - Alloy runs as a statefulset, with required tolerations in order to scrape metrics
networkPolicy:
  cilium:
    egress:
    - toEntities:
      - kube-apiserver
      - cluster
      - world
    - toEndpoints:
      - matchLabels:
          k8s-app: coredns
      - matchLabels:
          k8s-app: k8s-dns-node-cache
      toPorts:
      - ports:
        - port: "1053"
          protocol: UDP
        - port: "1053"
          protocol: TCP
        - port: "53"
          protocol: UDP
        - port: "53"
          protocol: TCP
    - toEndpoints:
      - matchLabels:
          app.kubernetes.io/instance: alloy-metrics
          app.kubernetes.io/name: alloy
      toPorts:
      - ports:
        - port: "12345"
          protocol: TCP
    ingress:
    - fromEndpoints:
      - matchLabels:
          app.kubernetes.io/instance: alloy-metrics
          app.kubernetes.io/name: alloy
      toPorts:
      - ports:
        - port: "12345"
          protocol: TCP
alloy:
  alloy:
    clustering:
      enabled: true
    configMap:
      create: true
      content: |-
        {{ .AlloyConfig | nindent 8 }}
    {{- if .IsSupportingVPA }}
    # We decided to configure the alloy-metrics resources as such after some investigation done https://github.com/giantswarm/giantswarm/issues/32655#issuecomment-2729636063
    resources:
      limits:
        cpu: 100m
        memory: 768Mi
      requests:
        cpu: 25m
        memory: 512Mi
    {{- end }}
  controller:
    type: statefulset
    replicas: {{ .Replicas }}
    priorityClassName: {{ .PriorityClassName }}
  crds:
    create: false
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - preference:
          matchExpressions:
          - key: karpenter.sh/capacity-type
            operator: NotIn
            values:
            - spot
        weight: 100
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/name
              operator: In
              values:
              - alloy
          topologyKey: kubernetes.io/hostname
        weight: 50
  {{- if .AlloyImageTag }}
  image:
    tag: {{ .AlloyImageTag }}
  {{- end }}
  stabilityLevel: experimental
{{- if .IsSupportingVPA }}
# We decided to configure the alloy-metrics vertical pod autoscaler as such after some investigation done https://github.com/giantswarm/giantswarm/issues/32655#issuecomment-2729636063
verticalPodAutoscaler:
  enabled: true
  resourcePolicy:
    containerPolicies:
    - containerName: alloy
      controlledResources:
      - memory
      - cpu
      controlledValues: "RequestsAndLimits"
      maxAllowed: 
        cpu: 500m
        memory: 10Gi
      minAllowed:
        cpu: 25m
        memory: 512Mi
{{- end }}
