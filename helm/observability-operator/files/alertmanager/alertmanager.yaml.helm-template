global:
  resolve_timeout: 5m
  http_config:
    proxy_from_environment: true
  slack_api_url: "https://slack.com/api/chat.postMessage"
  pagerduty_url: "https://events.eu.pagerduty.com/v2/enqueue"

templates:
  - notification-template.tmpl
  - url-template.tmpl

time_intervals:
  - name: business-hours
    time_intervals:
    - times:
      - start_time: 08:00
        end_time: 18:00
      weekdays: ['monday:friday']
      location: "Europe/Berlin"

route:
  group_by: [alertname, cluster_id, installation, status, team]
  group_interval: 15m
  group_wait: 5m
  repeat_interval: 4h
  receiver: root

  routes:
  - receiver: heartbeat
    matchers:
    - alertname="Heartbeat"
    continue: true
    group_wait: 30s
    group_interval: 30s
    repeat_interval: 15m
  {{- if eq .Values.managementCluster.pipeline "stable-testing" }}
  - receiver: blackhole
    matchers:
    - cluster_type="workload_cluster"
    continue: false
  - receiver: blackhole
    matchers:
    - cluster_id=~"t-.*"
    continue: false
  - receiver: blackhole
    matchers:
    - alertname=~"ClusterUnhealthyPhase|WorkloadClusterApp.*"
    continue: false
  - receiver: blackhole
    matchers:
    - alertname="ClusterUnhealthyPhase"
    - name=~"t-.*"
    continue: false
  # We don't want to get alerts by workload cluster apps that are failing.
  # We select those by checking if the App CR is in a namespace starting with 'org-'.
  - receiver: blackhole
    matchers:
    - alertname="ManagementClusterAppFailed"
    - namespace=~"org-([^g]|g[^i]|gi[^a]|gia[^n]|gian[^t]|giant[^s]|giants[^w]|giantsw[^a]|giantswa[^r]|giantswar[^m])+"
    continue: false
  {{- end }}

  # Falco noise Slack
  - receiver: falco_noise_slack
    matchers:
    - alertname=~"Falco.*"
    continue: false

  - receiver: team_atlas_github
    matchers:
    - severity=~"ticket"
    - team="atlas"
    continue: true

  - receiver: team_phoenix_github
    matchers:
    - severity=~"ticket"
    - team="phoenix"
    continue: true

  - receiver: team_shield_github
    matchers:
    - severity=~"ticket"
    - team="shield"
    continue: true

  - receiver: team_rocket_github
    matchers:
    - severity=~"ticket"
    - team="rocket"
    continue: true

  - receiver: team_honeybadger_github
    matchers:
    - severity=~"ticket"
    - team="honeybadger"
    continue: true

  - receiver: team_tenet_github
    matchers:
    - severity=~"ticket"
    - team="tenet"
    continue: true

  - receiver: team_tenet_slack
    repeat_interval: 14d
    matchers:
    - severity=~"notify"
    - team=~"tenet|tinkerers"
    continue: false

{{- if .Values.monitoring.opsgenieApiKey }}
  # Team Ops Opsgenie
  - receiver: opsgenie_router
    matchers:
    - severity="page"
    continue: true
{{- end }}

{{- range $.Values.alerting.teams }}

  {{- if not .name }}
  {{- fail "team must contain a name" }}
  {{- end }}

  {{- if .token }}

  {{- if eq $.Values.managementCluster.pipeline "stable" }}
  # PagerDuty pipeline=stable
  - receiver: pagerduty-{{ .name }}
    matchers:
    - severity="page"
    - pipeline="stable"
    - all_pipelines!="true"
    - team="{{ .team }}"
    continue: true
  {{- end }}

  {{- if eq $.Values.managementCluster.pipeline "stable-testing" }}
  # PagerDuty pipeline=stable-testing
  - receiver: pagerduty-{{ .name }}
    matchers:
    - severity="page"
    - pipeline="stable-testing"
    - all_pipelines!="true"
    - team="{{ .team }}"
    active_time_intervals:
    - business-hours
    continue: true
  {{- end }}

  # PagerDuty all-pipelines
  - receiver: pagerduty-{{ .name }}
    matchers:
    - all_pipelines="true"
    - team="{{ .team }}"
    continue: true

  # PagerDuty can't handle heartbeats for now
  #- receiver: pagerduty-{{ .name }}
  #  matchers:
  #  - alertname="Heartbeat"
  #  continue: true
  #  group_wait: 30s
  #  group_interval: 30s
  #  repeat_interval: 15m
  {{- end }}
{{- end }}

  # Team Atlas Slack
  - receiver: team_atlas_slack
    matchers:
    {{- if eq .Values.managementCluster.pipeline "stable" }}
    - severity="notify"
    {{- else }}
    - severity=~"page|notify"
    {{- end }}
    - team="atlas"
    - alertname!~"Inhibition.*"
    - alertname!="Heartbeat"
    continue: false

  # Team Phoenix Slack
  - receiver: team_phoenix_slack
    matchers:
    - team="phoenix"
    - sloth_severity="page"
    - silence="true"
    continue: false

  # Team Shield Slack
  - receiver: team_shield_slack
    matchers:
    - severity=~"page|notify"
    - team="shield"
    continue: false

  # Team Rocket Slack
  - receiver: team_rocket_slack
    matchers:
    - severity=~"page|notify"
    - team="rocket"
    continue: false

  # Team Honeybadger Slack
  - receiver: team_honeybadger_slack
    matchers:
    - severity=~"page|notify"
    - team="honeybadger"
    continue: false

receivers:
- name: root

{{- if .Values.monitoring.opsgenieApiKey }}
- name: heartbeat
  webhook_configs:
  - send_resolved: false
    http_config:
      authorization:
        type: GenieKey
        credentials: {{ .Values.monitoring.opsgenieApiKey }}
      follow_redirects: true
      enable_http2: true
      proxy_from_environment: true
    url: https://api.opsgenie.com/v2/heartbeats/{{ .Values.managementCluster.name }}/ping
{{- end }}

{{- if eq .Values.managementCluster.pipeline "stable-testing" }}
- name: blackhole
{{- end }}

- name: falco_noise_slack
  slack_configs:
  - channel: '#noise-falco'
    http_config:
      authorization:
        type: Bearer
        credentials: {{ .Values.alerting.slackAPIToken }}
      proxy_from_environment: true
    send_resolved: true
    actions: &slack-actions
    - type: button
      text: ':green_book: Runbook'
      url: '{{`{{ template "__runbook_url" . }}`}}'
      style: '{{`{{ if eq .Status "firing" }}primary{{ else }}default{{ end }}`}}'
    - type: button
      text: ':coffin: Linked PMs'
      url: '{{`{{ template "__alert_linked_postmortems" . }}`}}'
    - type: button
      text: ':mag: Query'
      url: '{{`{{ template "__alert_url" . }}`}}'
    - type: button
      text: ':grafana: Dashboard'
      url: '{{`{{ template "__dashboard_url" . }}`}}'
    - type: button
      text: ':no_bell: Silence'
      url: '{{`{{ template "__alert_silence_link" .}}`}}'
      style: '{{`{{ if eq .Status "firing" }}danger{{ else }}default{{ end }}`}}'

- name: team_atlas_slack
  slack_configs:
  {{- if eq .Values.managementCluster.pipeline "stable" }}
  - channel: '#alert-atlas'
  {{- else }}
  - channel: '#alert-atlas-test'
  {{- end }}
    http_config:
      authorization:
        type: Bearer
        credentials: {{ .Values.alerting.slackAPIToken }}
      proxy_from_environment: true
    send_resolved: true
    actions: *slack-actions

- name: team_phoenix_slack
  slack_configs:
  {{- if eq .Values.managementCluster.pipeline "stable" }}
  - channel: '#alert-phoenix'
  {{- else }}
  - channel: '#alert-phoenix-test'
  {{- end }}
    http_config:
      authorization:
        type: Bearer
        credentials: {{ .Values.alerting.slackAPIToken }}
      proxy_from_environment: true
    send_resolved: true
    actions: *slack-actions

- name: team_rocket_slack
  slack_configs:
  {{- if eq .Values.managementCluster.pipeline "stable" }}
  - channel: '#alert-rocket'
  {{- else }}
  - channel: '#alert-rocket-test'
  {{- end }}
    http_config:
      authorization:
        type: Bearer
        credentials: {{ .Values.alerting.slackAPIToken }}
      proxy_from_environment: true
    send_resolved: true
    actions: *slack-actions

- name: team_shield_slack
  slack_configs:
  - channel: '#alert-shield'
    http_config:
      authorization:
        type: Bearer
        credentials: {{ .Values.alerting.slackAPIToken }}
      proxy_from_environment: true
    send_resolved: true
    actions: *slack-actions

- name: team_tenet_slack
  slack_configs:
  - channel: '#alert-tenet'
    http_config:
      authorization:
        type: Bearer
        credentials: {{ .Values.alerting.slackAPIToken }}
      proxy_from_environment: true
    send_resolved: true
    actions: *slack-actions

- name: team_honeybadger_slack
  slack_configs:
  - channel: '#alert-honeybadger'
    http_config:
      authorization:
        type: Bearer
        credentials: {{ .Values.alerting.slackAPIToken }}
      proxy_from_environment: true
    send_resolved: true
    actions: *slack-actions

- name: team_atlas_github
  webhook_configs:
  - url: "http://localhost:8081/v1/webhook?owner=giantswarm&repo=giantswarm&labels=team/atlas"

- name: team_phoenix_github
  webhook_configs:
  - url: "http://localhost:8081/v1/webhook?owner=giantswarm&repo=giantswarm&labels=team/phoenix"

- name: team_shield_github
  webhook_configs:
  - url: "http://localhost:8081/v1/webhook?owner=giantswarm&repo=giantswarm&labels=team/shield"

- name: team_rocket_github
  webhook_configs:
  - url: "http://localhost:8081/v1/webhook?owner=giantswarm&repo=giantswarm&labels=team/rocket"

- name: team_honeybadger_github
  webhook_configs:
  - url: "http://localhost:8081/v1/webhook?owner=giantswarm&repo=giantswarm&labels=team/honeybadger"

- name: team_tenet_github
  webhook_configs:
  - url: "http://localhost:8081/v1/webhook?owner=giantswarm&repo=giantswarm&labels=team/tenet"

{{- if .Values.monitoring.opsgenieApiKey }}
- name: opsgenie_router
  opsgenie_configs:
  - api_key: {{ .Values.monitoring.opsgenieApiKey }}
    tags: '{{`{{ (index .Alerts 0).Labels.alertname }},{{ (index .Alerts 0).Labels.cluster_type }},{{ (index .Alerts 0).Labels.severity }},{{ (index .Alerts 0).Labels.team }},{{ (index .Alerts 0).Labels.area }},{{ (index .Alerts 0).Labels.service_priority }},{{ (index .Alerts 0).Labels.provider }},{{ (index .Alerts 0).Labels.installation }},{{ (index .Alerts 0).Labels.pipeline }},{{ (index .Alerts 0).Labels.customer }}`}}'
{{- end }}

{{- range $.Values.alerting.teams }}
{{- if .token }}
- name: pagerduty-{{ .name }}
  pagerduty_configs:
  # TODO: Replace opsgenie templates used here with something else.
  - routing_key: {{ .token }}
    source: '{{`{{ template "opsgenie.default.source" . }}`}}'
    description: '{{`{{ template "opsgenie.default.message" . }}`}}'
    # There is currently no way to send all alerts labels to PagerDuty as details.
    # See https://github.com/prometheus/alertmanager/issues/3218
    details: {
      _description: '{{`{{ template "opsgenie.default.description" . }}`}}',
      alertname: '{{`{{ .CommonLabels.alertname }}`}}',
      all_pipelines: '{{`{{ .CommonLabels.all_pipelines }}`}}',
      area: '{{`{{ .CommonLabels.area }}`}}',
      cluster_type: '{{`{{ .CommonLabels.cluster_type }}`}}',
      customer: '{{`{{ .CommonLabels.customer }}`}}',
      installation: '{{`{{ .CommonLabels.installation }}`}}',
      pipeline: '{{`{{ .CommonLabels.pipeline }}`}}',
      provider: '{{`{{ .CommonLabels.provider }}`}}',
      service_priority: '{{`{{ .CommonLabels.service_priority }}`}}',
      severity: '{{`{{ .CommonLabels.severity }}`}}',
      team: '{{`{{ .CommonLabels.team }}`}}',
      # This is to avoid the big text chunk about firing and resolved alerts.
      # We already have firing alerts informations templated in our description.
      firing: null,
      resolved: null
    }
{{- end }}
{{- end }}

inhibit_rules:
- source_matchers:
  - inhibit_kube_state_metrics_down=true
  target_matchers:
  - cancel_if_kube_state_metrics_down=true
  equal: [cluster_id]

- source_matchers:
  - cluster_control_plane_unhealthy=true
  target_matchers:
  - cancel_if_cluster_control_plane_unhealthy=true
  equal: [cluster_id]

- source_matchers:
  - cluster_control_plane_unhealthy=true
  target_matchers:
  - cancel_if_any_cluster_control_plane_unhealthy=true

- source_matchers:
  - kubelet_down=true
  target_matchers:
  - cancel_if_kubelet_down=true
  equal: [cluster_id, ip]

- source_matchers:
  - control_plane_node_down=true
  target_matchers:
  - cancel_if_control_plane_node_down=true
  equal: [cluster_id]

- source_matchers:
  - outside_working_hours=true
  target_matchers:
  - cancel_if_outside_working_hours=true

- source_matchers:
  - has_worker_nodes=false
  target_matchers:
  - cancel_if_cluster_has_no_workers=true
  equal: [cluster_id]

- source_matchers:
    - inhibit_monitoring_agent_down=true
  target_matchers:
    - cancel_if_monitoring_agent_down=true
  equal: [cluster_id]

# When a cluster looks broken
- source_matchers:
    - inhibit_cluster_broken=true
  target_matchers:
    - cancel_if_cluster_broken=true
  equal: [cluster_id]

# When metrics are unreliable (mimir broken)
- source_matchers:
    - inhibit_metrics_broken=true
  target_matchers:
    - cancel_if_metrics_broken=true

# Source: https://github.com/giantswarm/prometheus-rules/blob/main/helm/prometheus-rules/templates/kaas/tenet/alerting-rules/inhibit.nodes.rules.yml
- source_matchers:
    - node_not_ready=true
  target_matchers:
    - cancel_if_node_not_ready=true
  equal: [cluster_id, node]

# Source: https://github.com/giantswarm/prometheus-rules/blob/main/helm/prometheus-rules/templates/kaas/tenet/alerting-rules/inhibit.nodes.rules.yml
- source_matchers:
    - node_unschedulable=true
  target_matchers:
    - cancel_if_node_unschedulable=true
  equal: [cluster_id, node]
